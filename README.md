# Práctica de Dataframes con PySpark
![Práctica de Dataframes con PySpark en Databricks_ Análisis y Transformación de Datos](https://github.com/user-attachments/assets/b423cb90-575e-4856-9a84-6712d8a5f934)


Este repositorio contiene una práctica para aprender a trabajar con **Spark DataFrames** y SQL en un entorno distribuido utilizando **Databricks**. En esta práctica, aprenderás cómo manipular datos con DataFrames y cómo realizar consultas SQL sobre ellos, aprovechando las capacidades de procesamiento distribuido de Apache Spark.

El notebook de esta práctica se encuentra disponible en este repositorio, y está diseñado para guiarte paso a paso en la carga, transformación y análisis de datos utilizando Spark.

## Contenido

- Introducción al uso de **Spark DataFrames**.
- Carga de datos en **Databricks File System (DBFS)**.
- Manipulación de datos con **DataFrames**.
- Consultas y transformaciones utilizando **SQL** en Spark.
- Ejemplos prácticos de operaciones comunes en DataFrames.

## Requisitos previos

Antes de comenzar con la práctica, asegúrate de haber configurado tu entorno en Databricks. Aquí tienes algunos pasos previos que debes seguir:

1. **Verifica que tengas acceso a Databricks**: Asegúrate de tener una cuenta activa en Databricks y acceso a tu espacio de trabajo.

2. **Crea un clúster en Databricks**:  
   Si aún no has creado un clúster, es necesario hacerlo. Un clúster te permitirá ejecutar el código distribuido en un entorno Spark. Para aprender a configurar tu clúster, puedes seguir esta [guía paso a paso sobre la instalación y configuración de clústeres en Databricks](https://leonardonarvaez.com/blog/detail/guia-completa-de-databricks-instalacion-configuracion-de-clusteres-y-notebooks/).

3. **Configura tu notebook en Databricks**:  
   Crea un nuevo notebook en Databricks y conéctalo al clúster que creaste previamente.

4. **Carga los datos en DBFS (Databricks File System)**:  
   Antes de trabajar con los datos, necesitas cargarlos en DBFS. Puedes hacerlo desde la opción **"Data"** en Databricks, seleccionando **"Add Data"** para cargar archivos desde tu computadora o desde otras fuentes.

## Uso del Notebook

Una vez hayas cargado el notebook dentro de Databricks, sigue los pasos indicados en el mismo. En el notebook encontrarás explicaciones detalladas y ejemplos de código que te guiarán a través de las distintas transformaciones y operaciones que puedes realizar sobre los datos.

Te recomiendo que ejecutes cada celda del notebook por tu cuenta para familiarizarte con el flujo de trabajo y entender cómo interactuar con los datos. Puedes experimentar con los ejemplos, modificar el código y probar diferentes transformaciones para obtener una comprensión más profunda del uso de Spark DataFrames y SQL.

## Repositorio

El contenido de esta práctica, incluido el notebook que utilizamos, se encuentra disponible en este repositorio. Puedes clonar este repositorio o revisar el código directamente aquí:

- [Repositorio en GitHub](https://github.com/leo-narvaez/dataframes-spark)

## Contribuciones

Si tienes sugerencias o mejoras para este repositorio, siéntete libre de abrir un *pull request* o dejar un *issue*. Las contribuciones son bienvenidas.

## Licencia

Este proyecto está bajo la Licencia MIT. Consulta el archivo [LICENSE](LICENSE) para más detalles.
