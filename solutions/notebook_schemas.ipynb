{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "251a50f5-61e8-4c9f-9d32-fc920e806cc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Uso de esquemas para acelerar la lectura en Spark DataFrames\n",
    "\n",
    "Si bien Spark es lo mejor desde que se inventó el pan de molde para trabajar con grandes cantidades de datos, definitivamente me doy cuenta de que tengo mucho que aprender antes de poder aprovechar todo su potencial. Un truco que descubrí recientemente fue usar esquemas explícitos para acelerar la velocidad con la que PySpark puede leer un CSV en un DataFrame.\n",
    "\n",
    "Al utilizar PySpark `spark.read_csv` para leer un CSV , la forma más sencilla es establecer el `inferSchema` argumento en `True`. Esto significa que PySpark intentará verificar los datos para determinar qué tipo de datos es cada columna.\n",
    "\n",
    "El problema con esta operación es que consume mucha memoria, especialmente para conjuntos de datos grandes, ya que Spark necesita analizar una cantidad suficiente de datos para inferir correctamente el tipo. Imagine que tiene una columna con números enteros en las primeras 1000 filas, pero luego una cadena en la fila 1001. Si PySpark hubiera inferido que esta columna se basaba `IntegerType` en las primeras filas, terminaríamos con valores faltantes para cada una de las filas que contienen una cadena. Por lo tanto, PySpark necesita escanear todo el conjunto de datos o tomar muestras aleatorias de suficientes filas para inferir el tipo.\n",
    "\n",
    "Una forma de solucionar este problema es determinar el tipo de datos de antemano y pasar esta información a PySpark mediante un esquema. La sintaxis para esto es bastante sencilla. El nombre y el tipo de cada columna se definen mediante el `StructField` método `from pyspark.sql`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e16e867-96b1-4de8-b891-ee8480eb0197",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3629425818905074>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mStructField\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrip_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'StructField' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3629425818905074>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mStructField\u001B[49m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrip_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)\n\n\u001B[0;31mNameError\u001B[0m: name 'StructField' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'StructField' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "StructField(\"trip_id\", StringType(), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc1fa2ca-2d96-49b1-bdf7-e4b9e2256fb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los tres argumentos que `StructField` toma son el nombre que le gustaría darle a la columna, el tipo de datos de la columna y si el campo puede contener valores nulos (como un booleano). Los tipos de datos de columna disponibles también están en `pyspark.sql`, y cubren una amplia variedad de tipos de datos posibles, desde cadenas, flotantes y enteros hasta booleanos y de fecha y hora. Se crea un `StructField` para cada columna y estos se pasan como una lista a `pyspark.sql`. Luego `StructType`, este esquema se puede pasar al argumento `spark.read.csv` de `schema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9987d60-f1ab-4168-a484-1060f7b93ee7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3629425818905073>, line 1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mStructType\u001B[49m(\n",
       "\u001B[1;32m      2\u001B[0m     [StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrip_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n",
       "\u001B[1;32m      3\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcall_type\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)]\n",
       "\u001B[1;32m      4\u001B[0m )\n",
       "\n",
       "\u001B[0;31mNameError\u001B[0m: name 'StructType' is not defined"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3629425818905073>, line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mStructType\u001B[49m(\n\u001B[1;32m      2\u001B[0m     [StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrip_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[1;32m      3\u001B[0m     StructField(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcall_type\u001B[39m\u001B[38;5;124m\"\u001B[39m, StringType(), \u001B[38;5;28;01mTrue\u001B[39;00m)]\n\u001B[1;32m      4\u001B[0m )\n\n\u001B[0;31mNameError\u001B[0m: name 'StructType' is not defined",
       "errorSummary": "<span class='ansi-red-fg'>NameError</span>: name 'StructType' is not defined",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "StructType(\n",
    "    [StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"call_type\", StringType(), True)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f87099-36fc-4534-9a72-66027bb0153d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Para probar cuánto más rápido es usar un esquema, utilizaré el conjunto de datos [Taxi Service Trajectory](https://archive.ics.uci.edu/dataset/339/taxi+service+trajectory+prediction+challenge+ecml+pkdd+2015) del Repositorio de aprendizaje automático de UCI . Este conjunto de datos tiene 9 columnas y alrededor de 1,7 millones de filas. Vamos a cargar el contenido train.csv desde el DBFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "135a3c23-8f85-4e7c-b601-71c943f09443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from time import time\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() # SparkSession de forma programativa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0ab0d6b-d291-49fd-8153-42ee1e03796e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Creamos un Dataframe para guardar nuestro `train.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79bccd27-7d4d-483d-9b22-5e232ad30bcb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Veamos primero cuánto tiempo lleva leer estos datos cuando PySpark tiene que inferir el esquema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da50f18c-4ccc-4169-b58c-0151753d62e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 31.851423740386963 sec.\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\n|            TRIP_ID|CALL_TYPE|ORIGIN_CALL|ORIGIN_STAND| TAXI_ID| TIMESTAMP|DAY_TYPE|MISSING_DATA|            POLYLINE|\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\n|1372636858620000589|        C|       NULL|        NULL|20000589|1372636858|       A|       false|[[-8.618643,41.14...|\n|1372637303620000596|        B|       NULL|           7|20000596|1372637303|       A|       false|[[-8.639847,41.15...|\n|1372636951620000320|        C|       NULL|        NULL|20000320|1372636951|       A|       false|[[-8.612964,41.14...|\n|1372636854620000520|        C|       NULL|        NULL|20000520|1372636854|       A|       false|[[-8.574678,41.15...|\n|1372637091620000337|        C|       NULL|        NULL|20000337|1372637091|       A|       false|[[-8.645994,41.18...|\n|1372636965620000231|        C|       NULL|        NULL|20000231|1372636965|       A|       false|[[-8.615502,41.14...|\n|1372637210620000456|        C|       NULL|        NULL|20000456|1372637210|       A|       false|[[-8.57952,41.145...|\n|1372637299620000011|        C|       NULL|        NULL|20000011|1372637299|       A|       false|[[-8.617563,41.14...|\n|1372637274620000403|        C|       NULL|        NULL|20000403|1372637274|       A|       false|[[-8.611794,41.14...|\n|1372637905620000320|        C|       NULL|        NULL|20000320|1372637905|       A|       false|[[-8.615907,41.14...|\n|1372636875620000233|        C|       NULL|        NULL|20000233|1372636875|       A|       false|[[-8.619894,41.14...|\n|1372637984620000520|        C|       NULL|        NULL|20000520|1372637984|       A|       false|[[-8.56242,41.168...|\n|1372637343620000571|        A|      31508|        NULL|20000571|1372637343|       A|       false|[[-8.618868,41.15...|\n|1372638595620000233|        C|       NULL|        NULL|20000233|1372638595|       A|       false|[[-8.608716,41.15...|\n|1372638151620000231|        C|       NULL|        NULL|20000231|1372638151|       A|       false|[[-8.612208,41.14...|\n|1372637610620000497|        B|       NULL|          13|20000497|1372637610|       A|       false|[[-8.585145,41.16...|\n|1372638481620000403|        B|       NULL|          28|20000403|1372638481|       A|       false|[[-8.584263,41.16...|\n|1372639135620000570|        A|      33180|        NULL|20000570|1372639135|       A|       false|[[-8.666757,41.17...|\n|1372637482620000005|        C|       NULL|        NULL|20000005|1372637482|       A|       false|[[-8.599239,41.14...|\n|1372639181620000089|        C|       NULL|        NULL|20000089|1372639181|       A|       false|[[-8.643807,41.16...|\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo CSV que ya tienes\n",
    "data_location = \"/FileStore/dataframes-spark/train.csv\"\n",
    "\n",
    "# Iniciar el temporizador para medir el tiempo\n",
    "t1 = time()\n",
    "\n",
    "# Leer el archivo CSV con inferencia de esquema y encabezado\n",
    "data_inferred = spark.read.csv(data_location, header='True', inferSchema=True)\n",
    "\n",
    "# Medir el tiempo final\n",
    "t2 = time()\n",
    "\n",
    "# Imprimir el tiempo que tardó en leer el archivo\n",
    "print('Completed in %s sec.' % (str(t2 - t1)))\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar los datos\n",
    "data_inferred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dc01b1f7-0283-4ef0-a199-f1a14a417dd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Ahora compararemos cuánto tiempo lleva cuando le decimos explícitamente a PySpark cuál es el esquema de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fda7cb9-295a-4f83-b8ac-9cc4b78d4b57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed in 0.3081221580505371 sec.\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\n|            trip_id|call_type|origin_call|origin_stand| taxi_id| timestamp|day_type|missing_data|            polyline|\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\n|            TRIP_ID|CALL_TYPE|       NULL|        NULL|    NULL|      NULL|DAY_TYPE|        NULL|            POLYLINE|\n|1372636858620000589|        C|       NULL|        NULL|20000589|1372636858|       A|       false|[[-8.618643,41.14...|\n|1372637303620000596|        B|       NULL|           7|20000596|1372637303|       A|       false|[[-8.639847,41.15...|\n|1372636951620000320|        C|       NULL|        NULL|20000320|1372636951|       A|       false|[[-8.612964,41.14...|\n|1372636854620000520|        C|       NULL|        NULL|20000520|1372636854|       A|       false|[[-8.574678,41.15...|\n|1372637091620000337|        C|       NULL|        NULL|20000337|1372637091|       A|       false|[[-8.645994,41.18...|\n|1372636965620000231|        C|       NULL|        NULL|20000231|1372636965|       A|       false|[[-8.615502,41.14...|\n|1372637210620000456|        C|       NULL|        NULL|20000456|1372637210|       A|       false|[[-8.57952,41.145...|\n|1372637299620000011|        C|       NULL|        NULL|20000011|1372637299|       A|       false|[[-8.617563,41.14...|\n|1372637274620000403|        C|       NULL|        NULL|20000403|1372637274|       A|       false|[[-8.611794,41.14...|\n|1372637905620000320|        C|       NULL|        NULL|20000320|1372637905|       A|       false|[[-8.615907,41.14...|\n|1372636875620000233|        C|       NULL|        NULL|20000233|1372636875|       A|       false|[[-8.619894,41.14...|\n|1372637984620000520|        C|       NULL|        NULL|20000520|1372637984|       A|       false|[[-8.56242,41.168...|\n|1372637343620000571|        A|      31508|        NULL|20000571|1372637343|       A|       false|[[-8.618868,41.15...|\n|1372638595620000233|        C|       NULL|        NULL|20000233|1372638595|       A|       false|[[-8.608716,41.15...|\n|1372638151620000231|        C|       NULL|        NULL|20000231|1372638151|       A|       false|[[-8.612208,41.14...|\n|1372637610620000497|        B|       NULL|          13|20000497|1372637610|       A|       false|[[-8.585145,41.16...|\n|1372638481620000403|        B|       NULL|          28|20000403|1372638481|       A|       false|[[-8.584263,41.16...|\n|1372639135620000570|        A|      33180|        NULL|20000570|1372639135|       A|       false|[[-8.666757,41.17...|\n|1372637482620000005|        C|       NULL|        NULL|20000005|1372637482|       A|       false|[[-8.599239,41.14...|\n+-------------------+---------+-----------+------------+--------+----------+--------+------------+--------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Importar las clases necesarias de PySpark\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, BooleanType\n",
    "\n",
    "t1 = time()\n",
    "\n",
    "schema = StructType(\n",
    "    [StructField(\"trip_id\", StringType(), True),\n",
    "    StructField(\"call_type\", StringType(), True),\n",
    "    StructField(\"origin_call\", IntegerType(), True),\n",
    "    StructField(\"origin_stand\", IntegerType(), True),\n",
    "    StructField(\"taxi_id\", LongType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"day_type\", StringType(), True),\n",
    "    StructField(\"missing_data\", BooleanType(), True),\n",
    "    StructField(\"polyline\", StringType(), True)]\n",
    ")\n",
    "data_schema = spark.read.csv(data_location, header = 'False', schema = schema)\n",
    "\n",
    "t2 = time()\n",
    "print('Completed in %s sec.' % (str(t2 - t1)))\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar los datos\n",
    "data_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9638929c-1a7e-4125-aab9-814dceb58901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Los datos se leen aproximadamente 10 veces más rápido cuando le damos el esquema a PySpark en lugar de pedirle que lo deduzca. Obviamente, este es un paso más práctico cuando tienes datos con menos variables, pero cuando lees datos realmente grandes, es una manera fácil de ahorrar algo de tiempo de procesamiento."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "notebook_schemas",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
